# 8.クラスターインストール完了後の作業

これまでのステップでとりあえず`OpenShift`のクラスターとしては稼働するようになりました。コンテナもデプロイする事ができます。

ですが、現実の**使える**環境をを考えると、もう少しセットアップを続ける必要があります。

この章では、`PV(Persistent Volume)`用のストレージを準備し、運用のコンポーネントを載せるための`OpenShift`用語で`Infraノード`と呼ばれる`Workerノード`を作成します。

その後、`Container Registry`、`Elasticsearch(Cluster Logging)`を構成します。

これらの各種運用コンポーネントは、`Infraノード`に配置します。

また、`Prometheus`や、`Router(Ingress)`と言ったコンポーネントは、`OpenShift`クラスターインストール時に既に作成されていますが、これらも運用コンポーネントですので、`Infraノード`に移動させます。

![image.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/99425/2586aec7-8493-2101-434d-a65b50c398f7.png)



## 8.1. PV用のストレージの作成

`PV`用のストレージに何を使うかは環境次第ですが、この手順では、`OCS`を作成する事にします。以下の手順を参考に`OCS(OpenShift Container Storage)`を作成します。

<a href="https://qiita.com/Yuhkih/private/89e0c76703a4f90bae67">OCS(OpenShift Container Storage)をインターナルモードで作成する</a>

## 8.2.Infraノードの作成

`OpenShift`の運用コンポーネントを配置するための`Infraノード`を作成します。

`Infraノード`は、あくまで`OpenShift`の世界での用語です。技術的には`Workerノード`と同じものです。
`Workerノード`を新規に3本作成します。IPアドレスなどは、セクション[「1.1.7.ノードのサイジング」](#117ノードのサイジング)に書いてある通りです。

手順になれていれば、`Workerノード`の作成時に同時に`Infraノード`を作成しても良いのですが、作業に慣れていない時に、一度に大量の`Workerノード`を作成すると作業ミスした時のリカバリーが大変なので敢えて後から追加で構成する手順にしました。

まずは、通常の`Workerノード`と同様に以下の手順を実行します。

1.`Infraノード`用のIPアドレスを決定
2.`DHCPサーバー`に、MACアドレスに結びつけて`Infraノード`に固定IPを配れるように設定
3.`DNSサーバー`に`Infraノード`として使用する`Workerノード`のホスト名を DNSに登録 (逆引きも設定)
4.`CoreOS`のインストール。`Workerノード`の `Inginiton file`を使って `Workerノード`として構成
5.`Woker Node`の`CSR`を`approve`

ここでは、`i1.ocp45.example.localdomain`、`i2.ocp45.example.localdomain`、`i3.ocp45.example.localdomain`という3本の`Infraノード`を作成します。ここで作成する`Infraノード`のスペックは[1.1.7.1.OpenShift 関連ノードのサイジング](#1171openshift-関連ノードのサイジング)で設計した通りに、以下の通りです。

| サーバー        | vCPU(HT-on) | Memory      |  Disk   |    OS   |         ホスト名            | IP Address  |  note   | 
|:---------------|:--------------|:------------|:--------|:--------|:---------------------------|:------------|:-----|
|Infraノード      | 4VCPU         | 16GByte     | 120G    |  CoreOS |i1.ocp45.example.localdomain|172.16.0.41 |     | 
|                | 4VCPU         | 16GByte     | 120G    |  CoreOS |i2.ocp45.example.localdomain|172.16.0.42 |     | 
|                | 4VCPU         | 16GByte     | 120G    |  CoreOS |i3.ocp45.example.localdomain|172.16.0.43 |     | 

`Infraノード`の`CoreOS`のインストールに取りかかる前に、ネットワークの設定が取れているか確認しておく事をおすすめします。

```
dig +short i1.ocp45.example.localdomain   # ノードが無いと ping は返ってこないが、dig で名前解決はできる
dig +short i2.ocp45.example.localdomain   # ノードが無いと ping は返ってこないが、dig で名前解決はできる
dig +short i3.ocp45.example.localdomain   # ノードが無いと ping は返ってこないが、dig で名前解決はできる
dig +norec -x 172.16.0.41 +short     # Infraノード 1 のIPの逆引き
dig +norec -x 172.16.0.42 +short     # Infraノード 2 のIPの逆引き
dig +norec -x 172.16.0.43 +short     # Infraノード 3 のIPの逆引き
```



### 8.2.1.Infraノードとしてのラベルを貼る
`Workerノード`が完成したら、`Infraノード`としてのラベルを振ります。

```
# oc label node i1.ocp45.example.localdomain node-role.kubernetes.io/infra=
# oc label node i2.ocp45.example.localdomain node-role.kubernetes.io/infra=
# oc label node i3.ocp45.example.localdomain node-role.kubernetes.io/infra=
```

検索すると`Workerノード`としての`Role`を外す手順が出てくる場合がありますが、`worker` Role のラベルは外さなくても大丈夫です。
(`worker`ラベルを外す場合は、`Maching Config Pool`という追加リソースを作成する必要がでてきます。)

`infra`ラベルが貼られている事を確認します。

```
[root@bastion openshift]#  oc get nodes
NAME                           STATUS   ROLES          AGE   VERSION
i1.ocp45.example.localdomain   Ready    infra,worker   15h   v1.18.3+47c0e71
i2.ocp45.example.localdomain   Ready    infra,worker   15h   v1.18.3+47c0e71
i3.ocp45.example.localdomain   Ready    infra,worker   15h   v1.18.3+47c0e71
m1.ocp45.example.localdomain   Ready    master         15h   v1.18.3+47c0e71
m2.ocp45.example.localdomain   Ready    master         15h   v1.18.3+47c0e71
m3.ocp45.example.localdomain   Ready    master         15h   v1.18.3+47c0e71
s1.ocp45.example.localdomain   Ready    infra,worker   66m   v1.18.3+47c0e71
s2.ocp45.example.localdomain   Ready    infra,worker   66m   v1.18.3+47c0e71
s3.ocp45.example.localdomain   Ready    infra,worker   66m   v1.18.3+47c0e71
w1.ocp45.example.localdomain   Ready    worker         15h   v1.18.3+47c0e71
w2.ocp45.example.localdomain   Ready    worker         15h   v1.18.3+47c0e71
w3.ocp45.example.localdomain   Ready    worker         15h   v1.18.3+47c0e71
[root@bastion openshift]# 
```

### 8.2.2 Load Balancer の設定にInfraノードを追加する。

`Infraノード`に各種コンポーネントを移動させるので、この環境での`Load Balancer`である`HA Proxy`に、`Infraノード`を登録します。
`Load Balancer`である `HA Proxy`サーバーに移動して、`/etc/haproxy/haproxy.cfg`を編集します。

```/etc/haproxy/haproxy.cfg
<省略>
backend machine_config
    mode tcp
    balance     roundrobin
    server bs bs.ocp45.example.localdomain:22623 check
    server m1 m1.ocp45.example.localdomain:22623 check
    server m2 m2.ocp45.example.localdomain:22623 check
    server m3 m3.ocp45.example.localdomain:22623 check

backend worker_http
    mode tcp
    balance     source
    server i1 i1.ocp45.example.localdomain:80  check    # 追加
    server i2 i2.ocp45.example.localdomain:80  check    # 追加
    server i3 i3.ocp45.example.localdomain:80  check    # 追加
    server w1 w1.ocp45.example.localdomain:80  check
    server w2 w2.ocp45.example.localdomain:80  check
    server w3 w3.ocp45.example.localdomain:80  check

backend worker_https
    mode tcp
    balance     source
    server i1 i1.ocp45.example.localdomain:443  check    # 追加
    server i2 i2.ocp45.example.localdomain:443  check    # 追加
    server i3 i3.ocp45.example.localdomain:443  check    # 追加
    server w1 w1.ocp45.example.localdomain:443  check
    server w2 w2.ocp45.example.localdomain:443  check
    server w3 w3.ocp45.example.localdomain:443  check
```

設定を変更したら、有効化します。

```
systemctl restart haproxy
```

### 8.2.3 Infraノードに taint を付ける

以下のコマンドで、`Infraノード`に`taint`を付けます。

`taint`は、`Kubernetes`の`Pod`のスケジューリングを意図通りに行うために使われるテクニックの一つです。(参考：<a href="https://kubernetes.io/ja/docs/concepts/scheduling-eviction/taint-and-toleration/">TaintとToleration</a>)

`taint`を付ける前に、`OpenShift Console`の`Home`→`Overview`で`Alert`が上がってないか確認し作業前の状態に問題が無いか確認して下さい。

`taint`を付ける事でコンテナの移動が発生する事があり、一時的な警告などが表示される事があります。元から出ていた警告・エラーなのか、`taint`の追加により発生した警告・エラーなのか判別するために事前に状態を確認しておきます。

`taint`は、`Key=Value:Effect`の形式で表されますが、ここでは`Infraノード`に

`infra`=`reserved`:`NoSchedule` 
`infra`=`reserved`:`NoExecute`

の２つの`taint`を付けます。

```
oc adm taint node i1.ocp45.example.localdomain infra=reserved:NoSchedule infra=reserved:NoExecute
oc adm taint node i2.ocp45.example.localdomain infra=reserved:NoSchedule infra=reserved:NoExecute
oc adm taint node i3.ocp45.example.localdomain infra=reserved:NoSchedule infra=reserved:NoExecute
```

`taint`は以下のコマンドで確認できます。

```
oc describe node <node名>
```

```.yaml
<省略>
CreationTimestamp:  Sat, 10 Oct 2020 01:00:40 +0900
Taints:             infra=reserved:NoExecute
                    infra=reserved:NoSchedule
Unschedulable:      false
<省略>
```

`taint`を付ける事で、これまで`Infraノード`上に配置されていた、`toleration`の無い一部のコンテナが`Infraノード`外に追い出されます。
`OpenShift Console`の`Home`->`Overview`で状態が落ち着くまで`Alert`を確認して下さい。

特に問題が無さそうであれば`Infraノード`のセットアップは完了です。

## 8.3.Cluster Logging のセットアップ

この手順は`PV`を作成できるようになっている事が前提です。

`OpenShift`では、ロギングに`EKF(ElastictSearch/Fluented/Kibana)`スタックを使用しています。`OpenShift`ではこれをひとまとめに`Cluster Logging`と読んでいます。

以下の手順を参考に`Cluster Logging`をインストールします。独立した手順として成り立たせるために、`Infraノード`の作成についても書いてますが、その部分は、セクション[「8.2.infraノードの作成」](#82Infraノードの作成) と同じなのでスキップして大丈夫です。

<a href="https://qiita.com/Yuhkih/private/de109779f2365c37b57e">OpenShift 上に Cluster Logging (EFK) をインストールする</a>

## 8.4.Container Registry の作成

`OpenShift` 上に`OpenShift`標準の`Container Registry`を作成します。

この手順は、`OCS`がインターナルモード(`OpenShift`のクラスター内の`ノード`として稼働）で作成されている事が前提です。
(`OCS`で無いストレージを使用している場合は、`Storage Class`名が、使用するストレージが提供している`Storage Class`名になります）

### 8.4.1.Registry 用 Storage Class と PVC の実行

既存の`OCS`の`FileSystem`用の`StorageClass`の定義`ocs-storagecluster-cephfs`を元に、新しい`StroageClass`を作成します。

```
oc get sc ocs-storagecluster-cephfs -o yaml > my-sc-fs.yaml
```

上記で出力した`Storage Class`の定義を編集して、`Storage Class`名`my-registry-fs-sc`を作成します。
デフォルトで用意されている`Storage Class`は、`Reclaim Policy`が、`Delete`です。

一応、`Cotainer Registry`に保管されるデータは、その企業の資産とも言えるので、`Reclaim Policy`を`Retain`に書き替えた新しい`Storage Class`を作ります。

```my-sc-fs.yaml
allowVolumeExpansion: true
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: my-registry-fs-sc
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  fsName: ocs-storagecluster-cephfilesystem
provisioner: openshift-storage.cephfs.csi.ceph.com
reclaimPolicy: Retain    # ここを Delete から Retainに
volumeBindingMode: Immediate

```

作成した`my-sc-fs.yaml`を適用します。

```
 oc apply -f my-sc-fs.yaml
```

新しい`SC(Storage Class)`ができた事を確認します

```
# oc get sc
NAME                          PROVISIONER                             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
localblock                    kubernetes.io/no-provisioner            Delete          WaitForFirstConsumer   false                  2d12h
my-registry-fs-sc               openshift-storage.noobaa.io/obc         Retain          Immediate              false                  2m16s
ocs-storagecluster-ceph-rbd   openshift-storage.rbd.csi.ceph.com      Delete          Immediate              true                   2d12h
ocs-storagecluster-ceph-rgw   openshift-storage.ceph.rook.io/bucket   Delete          Immediate              false                  2d12h
ocs-storagecluster-cephfs     openshift-storage.cephfs.csi.ceph.com   Delete          Immediate              true                   2d12h
openshift-storage.noobaa.io   openshift-storage.noobaa.io/obc         Delete          Immediate              false                  2d12h
```

`Storage Class`名、`my-registry-fs-sc`が作成されています。

次に作成した`Storage Class`を使って`PV`を`Claim`するために、以下のような、`PVC(Persistent Volume Claim)`定義ファイル`my-pvc-fs.yaml `を作ります。`PVC`名は、`image-registry-storage`で、サイズは`100Gi`にしています。

```my-pvc-fs.yaml 
apiVersion: "v1"
kind: "PersistentVolumeClaim"
metadata:
  name: "image-registry-storage"  
spec:
  accessModes:
    - "ReadWriteMany"
  resources:
    requests:
      storage: "100Gi"
  storageClassName: "my-registry-fs-sc"   # 先ほど作成した Storage Class
```

以下のコマンドで、作成した`my-pvc-fs.yaml`を適用し`PVC`を作成します。

```
# oc project openshift-image-registry
# oc apply -f my-pvc-fs.yaml 
```

`PVC`が出来た事を確認します。

```
# oc get pvc -n openshift-image-registry
NAME                     STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS        AGE
image-registry-storage   Bound     pvc-4a3e0b7e-8b30-4753-98aa-95d92d244e26   100Gi      RWX            my-registry-fs-sc   4s
```

`Status`が`Bound`になっていればOKです。

### 8.4.2.Registryの構成

` configs.imageregistry.operator.openshift.io`の`managedState`を`Manged`に変更します。

```
oc patch configs.imageregistry.operator.openshift.io cluster --type merge --patch '{"spec":{"managementState":"Managed"}}'
```

念のため`openshift-image-registry`ネームスペースの`Pod`が問題なく稼働している事を確認します。

```
[root@bastion openshift]# oc get pod -n openshift-image-registry
NAME                                               READY   STATUS    RESTARTS   AGE
cluster-image-registry-operator-56d78bc5fb-fckwp   2/2     Running   0          19h
image-registry-58876b649-v48xk                     1/1     Running   0          5m51s
node-ca-c5v7p                                      1/1     Running   0          5h15m
node-ca-cxn2g                                      1/1     Running   0          19h
node-ca-db25w                                      1/1     Running   0          19h
node-ca-f2nk8                                      1/1     Running   0          19h
node-ca-gvng5                                      1/1     Running   0          19h
node-ca-gzh4m                                      1/1     Running   0          5h15m
node-ca-j7nxd                                      1/1     Running   0          19h
node-ca-k652z                                      1/1     Running   0          19h
node-ca-ls9rp                                      1/1     Running   0          19h
node-ca-s5v4f                                      1/1     Running   0          5h15m
node-ca-tg2hz                                      1/1     Running   0          19h
node-ca-vnq7j                                      1/1     Running   0          19h
[root@bastion openshift]# 
```

先ほど作成した`PVC`が、` configs.imageregistry.operator.openshift.io`から参照されるように設定します。

```
oc edit configs.imageregistry.operator.openshift.io
```

```.yaml
<省略>
spec:
  httpSecret: 4c0857824322cf083be5198d5ca486eeed9afb18c2870fcdef3a78efe21168e545a192ab6b1030c86870ef45072fcbcef0a359c0d435ad04f6cfa2475a0f027e
  logging: 2
  managementState: Managed
  proxy: {}
  replicas: 1
  requests:
    read:
      maxWaitInQueue: 0s
    write:
      maxWaitInQueue: 0s
  rolloutStrategy: RollingUpdate
  storage:                      # {} を削除
    pvc:                        # 追加
      claim: image-registry-storage  # 追加 (前段で作成したPVC名。空欄にした場合でも、この名前が自動でセットされる)
status:
  conditions:
  - lastTransitionTime: "2020-10-09T15:57:45Z"
    reason: AsExpected
    status: "False"
<省略>
```


`image-registry Operator`のステータスを確認します。

```
# oc get clusteroperator image-registry
NAME             VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
image-registry   4.5.13    True        False         True       6m17s
```

`AVAILABLE`が`True`になるまで待ちます。数分かかるはずです。

### 8.4.3.Container Registry のInfraノードへの移動


以下のコマンドで、`Container Regsitry`を`Infraノード`に移動させます。

```
oc patch configs.imageregistry.operator.openshift.io/cluster --type=merge -p '{"spec":{"nodeSelector": {"node-role.kubernetes.io/infra": ""},"tolerations": [{"effect":"NoSchedule","key": "infra","value": "reserved"},{"effect":"NoExecute","key": "infra","value": "reserved"}]}}'
```
ワンライナーでちょっと読みにくいですが、`nodeSelector`で`Infraノード`を指定し、`Infraノード`の`taint`に弾かれないように`tolerations`を付けています。

確認してみます。

```
[root@bastion openshift]# oc get pod -n openshift-image-registry
NAME                                               READY   STATUS      RESTARTS   AGE
cluster-image-registry-operator-56d78bc5fb-6kflj   2/2     Running     0          15h
image-pruner-1604016000-2hhx7                      0/1     Completed   0          6h34m
image-registry-58579b74c5-pbwr4                    1/1     Running     0          2m24s
node-ca-58d7m                                      1/1     Running     0          14h
node-ca-5h2x9                                      1/1     Running     0          5h32m
node-ca-5ngxk                                      1/1     Running     0          14h
node-ca-8cvd6                                      1/1     Running     0          14h
node-ca-92xtj                                      1/1     Running     0          14h
node-ca-cxn5g                                      1/1     Running     0          15h
node-ca-drgkr                                      1/1     Running     0          15h
node-ca-jdlrd                                      1/1     Running     0          5h32m
node-ca-jtsls                                      1/1     Running     0          5h32m
node-ca-n9gl5                                      1/1     Running     0          14h
node-ca-qbwqc                                      1/1     Running     0          14h
node-ca-zvtq8                                      1/1     Running     0          15h
[root@bastion ~]#
```

どのコンポーネントが何かというのは、見た目で判断するしかないのですが、各ノード毎に置かれているコンポーネント以外の配置について確認してみます。


```
[root@bastion openshift]# oc describe pods cluster-image-registry-operator-56d78bc5fb-6kflj -n openshift-image-registry | grep Node:
Node:                 m2.ocp45.example.localdomain/172.16.0.22
[root@bastion openshift]# oc describe pods image-registry-58579b74c5-pbwr4  -n openshift-image-registry | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion openshift]# oc describe pods image-pruner-1604016000-2hhx7   -n openshift-image-registry | grep Node:
Node:         s3.ocp45.example.localdomain/172.16.0.53
```

`cluster-image-registry-operator-xxxx`は、`Masterノード`上に配置されていますが、`Controller`だと思われるので、これで良い事にします。
`image-pruner-xxxx`が、`Workerノード`上で実行された形跡がありますが、これは一時的な`Pod`なので後で問題なさそうですが、後で追求してみます。

この例だと`image-registry-58876b649-v48xk`が、`Infraノード`上にあれば問題なさそうなので、これで良い事にします。


## 8.5. RouterのInfraノードへの移動

この手順は、`Infraノード`が作成されている事が前提になります。

### 8.5.1.nodeSelectorとtolerationの設定

`Infraノード`に、Podをスケジュールするには、`nodeSelector`で、`Infraノード`を選択させます。

また`Infraノード`にユーザーアプリケーションがスケジュールされないように、最終的に`Infraノード`に`taint`を付けます。その`taint`を無視できるように`toleration`も`Infraノード`にスケジュールするコンポーネントに付ける必要があります。

まとめると、`Infraノード`にスケジュールする、コンポーネントには、以下の記述を`spec`に追加します。

```
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/infra: ""
    tolerations:
    - effect: NoSchedule
      key: infra
      value: reserved
    - effect: NoExecute
      key: infra
      value: reserved
```

`Router`に上記の`nodeSelector`と`toleration`を追加するために以下のコマンドを実行します。

```
oc patch ingresscontroller/default -n  openshift-ingress-operator  --type=merge -p '{"spec":{"nodePlacement": {"nodeSelector": {"matchLabels": {"node-role.kubernetes.io/infra": ""}},"tolerations": [{"effect":"NoSchedule","key": "infra","value": "reserved"},{"effect":"NoExecute","key": "infra","value": "reserved"}]}}}'
```

さらにデフォルトでは、`Router`は、2つしか Replica を持たないので、`Infraノード`の数に合わせるため3つにします。

```
oc patch ingresscontroller/default -n openshift-ingress-operator --type=merge -p '{"spec":{"replicas": 3}}'
```

### 8.5.2.スケジュールの確認

`oc describe node | grep ingress` で、`ingress`関連のコンポーネントの名前を見つけます。

```
root@bastion openshift]# oc describe node | grep ingress
  openshift-ingress                       router-default-cbbf5f64-fxpzf                   100m (2%)     0 (0%)      256Mi (0%)       0 (0%)         4m25s
  openshift-ingress                       router-default-cbbf5f64-m4mhp                    100m (2%)     0 (0%)      256Mi (0%)       0 (0%)         4m45s
  openshift-ingress                       router-default-cbbf5f64-g4tvj                    100m (2%)     0 (0%)      256Mi (0%)       0 (0%)         4m59s
  openshift-ingress-operator              ingress-operator-56f5778d85-ddncf                        20m (0%)      0 (0%)      40Mi (0%)        0 (0%)         15h
[root@bastion openshift]# 
```

一つ一つ配置を確認してみます。

```
[root@bastion openshift]# oc describe pod ingress-operator-56f5778d85-ddncf -n openshift-ingress-operator | grep Node:
Node:                 m2.ocp45.example.localdomain/172.16.0.22
[root@bastion openshift]# oc describe pod router-default-cbbf5f64-fxpzf -n openshift-ingress | grep Node:
Node:                 i1.ocp45.example.localdomain/172.16.0.41
[root@bastion openshift]# oc describe pod router-default-cbbf5f64-m4mhp -n openshift-ingress | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion openshift]# oc describe pod router-default-cbbf5f64-g4tvj -n openshift-ingress | grep Node:
Node:                 i3.ocp45.example.localdomain/172.16.0.43
[root@bastion openshift]# 
```

`ingress-operator-xxxx`は、`Masterノード`で、その他は`infraノード`なので問題なさそうです。

以上で、`Router`の設定は完了です。

`Router`の設定は以下のKBを参考にしました。
<a href="https://access.redhat.com/solutions/5034771">参考：Configuring Infrastructure Nodes with Taint in OpenShift 4</a>
このKBは、他のコンポーネントの`infraノード`へのスケジュール方法も書いてますが、気づいた範囲では`Prometheus`部分で、`Thanos`の設定が抜けているようです。

## 8.6. Prometheus の構成

`Prometheus`関連のコンポーネントは、`OpenShift`では`Cluster Monitoring`と呼ばれています。
`OpenShift`のインストールと一緒に`Prometheus`は既にインストールされて稼働しています。

ここでは`Prometheus`のメインコンポーネントを、`Infraノード`に移動させデータ保管用に`PV`を作成します。
この作業は`Prometheus`設定用の`ConfigMap`を作成する事で実現できます。

### 8.6.1. Prometheus 用のPVの作成とInfraノードへの移動

以下の`cluster-monitoring-configmap.yaml`を作成します。

`Promethues`の各コンポーネントに、`nodeSelector`を使って`Infraノード`に配置するよう指定する事と、`Infraノード`に付いている`taint`を無視するように`toleration`を付けた`ConfigMap`リソースを作成しています。

ここではログ保管用に`40Gi`の`PVC`をしていいます。**事前に`PVC`をする事で`PV`が作成される環境である事を想定しています。**

```cluster-monitoring-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector:            # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:              # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    prometheusK8s:
      volumeClaimTemplate:                            # volumeCliaimTemplate 
         spec:                                          # 追加
            storageClassName: ocs-storagecluster-cephfs  # Ceph FileSystem の Storage Class
            volumeMode: Filesystem                       # FileSystem
            resources:                                   # 追加
              requests:                                  # 追加
                  storage: 40Gi                             # Size はとりあえず 40Gi
      nodeSelector:                 # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                  # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    prometheusOperator:
      nodeSelector:                # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                 # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    grafana:
      nodeSelector:                 # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                   # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:                # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                  # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      nodeSelector:                # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                  # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      nodeSelector:               # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                 # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      nodeSelector:              # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      nodeSelector:              # nodeSelectorでinfraを選ぶ
        node-role.kubernetes.io/infra: ""
      tolerations:                # toleration を付ける
      - key: infra
        value: reserved
        effect: NoSchedule
      - key: infra
        value: reserved
        effect: NoExecute
```

作成したファイルを適用します。

```
oc create -f cluster-monitoring-configmap.yaml
```

`PVC`ができているか確認します。

```
# oc get pvc -n openshift-monitoring
NAME                                 STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS                AGE
prometheus-k8s-db-prometheus-k8s-0   Bound    pvc-78175f98-d22e-4023-96ac-6bf2a92b973b   40Gi       RWO            ocs-storagecluster-cephfs   36s
prometheus-k8s-db-prometheus-k8s-1   Bound    pvc-22975d1f-47ed-40a5-b1d6-5131c22dab36   40Gi       RWO            ocs-storagecluster-cephfs   35s
```

２つ`PVC`が出来ていて`Status`が`Bound`になっていればOKです。

各コンポーネントの配置を確認してみます。

```
[root@bastion ~]# oc get pods -n openshift-monitoring
NAME                                           READY   STATUS    RESTARTS   AGE
alertmanager-main-0                            5/5     Running   0          20h
alertmanager-main-1                            5/5     Running   0          20h
alertmanager-main-2                            5/5     Running   0          20h
cluster-monitoring-operator-79d58c4598-hqrc7   2/2     Running   2          39h
grafana-859c496b85-84rnh                       2/2     Running   0          20h
kube-state-metrics-ff7bbdd5c-bwjwt             3/3     Running   0          20h
node-exporter-2ptbn                            2/2     Running   0          39h　　<== これは Node毎にある。
node-exporter-65ckx                            2/2     Running   0          39h
node-exporter-94kpv                            2/2     Running   0          25h
node-exporter-9gfrm                            2/2     Running   0          39h
node-exporter-bmfmb                            2/2     Running   0          39h
node-exporter-g5pt6                            2/2     Running   0          25h
node-exporter-l7qsx                            2/2     Running   0          39h
node-exporter-mmw2f                            2/2     Running   0          39h
node-exporter-rvq5j                            2/2     Running   0          25h
node-exporter-rvs6h                            2/2     Running   0          39h
node-exporter-z7pbv                            2/2     Running   0          39h
node-exporter-zzhv6                            2/2     Running   0          39h
openshift-state-metrics-674b677b97-5m9bk       3/3     Running   0          20h
prometheus-adapter-ccfbd8b84-wbvbk             1/1     Running   0          16h
prometheus-adapter-ccfbd8b84-xx8mc             1/1     Running   0          16h
prometheus-k8s-0                               7/7     Running   0          5h18m
prometheus-k8s-1                               7/7     Running   0          5h19m
prometheus-operator-7bf95c767d-9b9wp           2/2     Running   0          20h
telemeter-client-76966457-8k5g6                3/3     Running   0          20h
thanos-querier-867c77f45d-d8gg2                4/4     Running       0          4m56s
thanos-querier-867c77f45d-k4h84                4/4     Running       0          5m15s
```

`ノード`毎に必要な`exporter`以外の配置を確認してみます。

```
[root@bastion ~]# oc describe pods alertmanager-main-0 -n openshift-monitoring | grep Node:
Node:                 i1.ocp45.example.localdomain/172.16.0.41
[root@bastion ~]# oc describe pods alertmanager-main-1 -n openshift-monitoring | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion ~]# oc describe pods alertmanager-main-2 -n openshift-monitoring | grep Node:
Node:                 i3.ocp45.example.localdomain/172.16.0.43
[root@bastion ~]# oc describe pods cluster-monitoring-operator-79d58c4598-hqrc7 -n openshift-monitoring | grep Node:
Node:                 m2.ocp45.example.localdomain/172.16.0.22
[root@bastion ~]# oc describe pods grafana-859c496b85-84rnh  -n openshift-monitoring | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion ~]# oc describe pods kube-state-metrics-ff7bbdd5c-bwjwt -n openshift-monitoring | grep Node:
Node:                 i1.ocp45.example.localdomain/172.16.0.41
[root@bastion ~]# oc describe pods kube-state-metrics-ff7bbdd5c-bwjwt -n openshift-monitoring | grep Node:
Node:                 i1.ocp45.example.localdomain/172.16.0.41
[root@bastion ~]# oc describe pods openshift-state-metrics-674b677b97-5m9bk -n openshift-monitoring | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion ~]# oc describe pods prometheus-adapter-ccfbd8b84-wbvbk -n openshift-monitoring | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion ~]# oc describe pods prometheus-adapter-ccfbd8b84-xx8mc -n openshift-monitoring | grep Node:
Node:                 i3.ocp45.example.localdomain/172.16.0.43
[root@bastion ~]# oc describe pods prometheus-k8s-0  -n openshift-monitoring | grep Node:
Node:                 i3.ocp45.example.localdomain/172.16.0.43
[root@bastion ~]# oc describe pods prometheus-k8s-1  -n openshift-monitoring | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion ~]# oc describe pods prometheus-operator-7bf95c767d-9b9wp -n openshift-monitoring | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion ~]# oc describe pods telemeter-client-76966457-8k5g6 -n openshift-monitoring | grep Node:
Node:                 i1.ocp45.example.localdomain/172.16.0.41
[root@bastion openshift]# oc describe pods thanos-querier-867c77f45d-d8gg2 -n openshift-monitoring | grep Node:
Node:                 i2.ocp45.example.localdomain/172.16.0.42
[root@bastion openshift]# oc describe pods thanos-querier-867c77f45d-k4h84 -n openshift-monitoring | grep Node:
Node:                 i1.ocp45.example.localdomain/172.16.0.41
[root@bastion openshift]# 
```

概ね必要なコンポーネントは、`Infraノード`に配置されているようです。

`cluster-monitoring-operator-xxx` が`Masterノード`に配置されていましたが、これは指定方法がマニュアルになかったのと、このままで良さそうなので設計通りなのかもしれません。

概ね`Infraノード`に配置されているという事で、良い事にします。

<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.5/html/monitoring/cluster-monitoring">参考: 1.1. About cluster monitoring</a>

## 8.7. Workerノード の Load Balancer からの削除

HTTP/HTTPS トラフィックは、この環境での`Load Balancer`である`HA Proxy`から、`Infraノード`上の`Router`で一旦受けた後、`Pod`に割り振られます。

`Workerノード`で`Load Balancer(HA Proxy)`から直接トラフィックを受け取る必要がないので、`HA Proxy`の設定から`Workerノード`を取り外します。

また、好みの問題ですが、xxxxx_**worker**http(s) だった部分 をxxxxx_**infra**http(s)に置き換えています。

```/etc/haproxy/haproxy.cfg

<省略>
 frontend infrahttp　　　　　　　 # 名前の変更
    default_backend infra_http   # 名前の変更
    mode tcp
    bind *:80

 frontend infrahttps　　　　　　　 # 名前の変更
    default_backend infra_https   # 名前の変更
    mode tcp
    bind *:443

<省略>

backend infra_http                          # 名前の変更
    mode tcp
    balance     source
    server i1 i1.ocp45.example.localdomain:80  check    
    server i2 i2.ocp45.example.localdomain:80  check   
    server i3 i3.ocp45.example.localdomain:80  check  

backend infra_https                           # 名前の変更
    mode tcp
    balance     source
    server i1 i1.ocp45.example.localdomain:443  check    
    server i2 i2.ocp45.example.localdomain:443  check    
    server i3 i3.ocp45.example.localdomain:443  check    
```

設定を変更したら、有効化します。

```
haproxy -f /etc/haproxy/haproxy.cfg -c  # 構成の確認
systemctl restart haproxy               # 再起動
```

現在、`OpenShift Console`の入り口となる`Router`(`OpenShift`の`Ingress`)は、`Infraノード`に配置されているはずなので、アクセスできるかどうか確認してみてください。(`OpneShift`の`Console`の`Pod`は、、`Masterノード`に配置されています。)
