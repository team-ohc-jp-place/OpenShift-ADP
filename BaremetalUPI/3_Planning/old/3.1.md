# 3. OpenShift Baremetal UPI : クラスター設計
本章では、`OpenShift`のBaremetal UPIでインストールするクラスターの設計について説明します。


## 3-1
![システム構成図](./images/image_3-1-01.png)

本章では上図のような構成の`OpenShift`クラスターを設計しています。

手元の実験環境を作るものというよりは、本番環境で最低限必要なコンポーネントを何かを考えて、冗長化が必要なコンポーネントは分割しています。

### 本構成の制限

`DNS(BIND)`、`LoadBalancer(HA Proxy)`は、本番環境であれば冗長化するべき所ですが、手順が冗長になってしまい複雑になってしまうため、この手順の中では、触れていません。

また ID管理のコンポーネントである`IdM` は、`OpenShift`の動作確認後、後から追加構築できる部分でもあるので、時間的都合もありこの手順書では触れていません(後から追加できれば追加します)。

### 本構成のメリット・デメリット

この構成のメリットは、実際のフルの運用環境と比べると**比較的**構成が小さい事です。

一方で、`OpenShift`ではクラスタの中に監視やログ収集のコンポーネントやストレージまでが含まれています。そのため、頻繁に発生する`Kuberentes`のアップデート時において、監視やログ収集がまともに機能しません。

`OpenShift`は、クリック一つで全体をバージョンアップできる`OTA(Over The Air)`アップデートができますが、監視システムもまとめてアップデートされるためアップデート中にいろいろな`Warning`やエラーが上がっても、状況がつかめませんでした。

もともと物理サーバーの時代より**監視サーバーは、監視対象と同じプラットフォームに同居させてはいけない**というのは先人の知恵から言われていた事ですが、実際には仮想化が普及しはじめてから、同じ仮想化プラットフォーム上に監視サーバーが載っている例も多かったと思います。

これは`vMontion`や`Live Migration`と言った`ハイパーバイザー`のアップデート時にコンポーネントを待避させる仕組みや、監視サーバーの分離度が仮想マシンという形で他と大きく分離しているという観点で、現実的に監視が停止する確率がそれほど高くなかったためだと思います。

一方で現状、3ヶ月に一回アップデートが走る`Kubernetes`の世界では、監視は監視対象が乗るプラットフォームと分割するという原則は本番環境では特に守った方が良いと思いました。

が、いずれにしてもこの手順では、リソースの都合で、そこまでの環境を整備する事までは行っておらず、`OpenShift`と`OpenShift`に含まれている`OSS`製品の構築方法、分散ストレージである`OCS(OpenShift Container Storage)` の構築の作業手順やその雰囲気を届ける事を目的としたいと思います。



以降はクラスターを構成するコンポーネントやサービスについて、それらを設計するポイントを説明します。

---

## 3.1 クラスターを構成するコンポーネント
OpenShiftクラスターは様々な要素によって構成されていますが、代表的なコンポーネントとして、

- ノード
- ネットワーク
- ストレージ

が挙げられます。<br>
本節では、これらの設計について説明します。

### 3.2.1. ノード
OCPクラスターは複数の種類のノードが複数台集まって構成されることが一般的です。<br>
ここでは各ノードについて設計します。

#### 3.2.1.1. ノード一覧
本ドキュメントでは合計13台のインスタンスをOCPのノードとして使います。それぞれのノードの種類とスペックは以下の通りです。

| ノード | 台数 | x86_64 CPU <br> (thread,vcpu)<sup>1</sup> | メモリ搭載量 | システムドライブ <sup>2</sup> | 追加ドライブ <sup>3</sup>| 10G NIC <br> (Ports) <sup>4</sup> | 備考 |
|:--------:|:--------:|:-----------------:|:----------------:|:----------------:|:------------:|:------------:|:-----------------|
| Master   | 3 | 8  | 32 GB | 120 GiB | -            | 1 |
| Worker   | 3 | 8  | 32 GB | 120 GiB | -            | 1 |
| Infra    | 3 | 8  | 32 GB | 120 GiB | -            | 1 |
| OCS      | 3 | 16 | 48 GB | 120 GiB | 1 TiB SSD x3 | 1 | 合計 3 TiB の Persistent Storage
| Bootstrap| 1 | 4  | 16 GB | 120 GiB | -            | 1 | クラスターが構築できたら<br>認証サーバ等別の役割に切り替える
<br>
&emsp; *1 : SMTを有効とする物理サーバー、または仮想サーバーを使う場合は、1 core = 2 threads = 2 vcpus で換算 <br>
&emsp; *2 : 必要に応じて適宜2ドライブでRAID 1を構成するなどして冗長化する <br>
&emsp; *3 : Persistent Storageとして必要となる容量を搭載する。詳細は下記サイジングのヒントを参照 <br>
&emsp; *4 : 必要に応じて適宜Bondingなどで冗長化、帯域幅の拡張を行う。BMC用は必要に応じて適宜用意する <br>

<br>
<br>
OCPクラスターのノードは、MasterノードとWorkerノードに大別され、Masterノード以外は全てWorkerノードとして扱われます。<br>
しかしInfraやOCS


- OpenShift クラスターは、Master Node と Worker Node で構成される。
- オプションで Infrastructure Node(Infra Node) と Storage Node(OCS) ノードも加えることで、よりOpenShiftの使用に適したクラスターになるため、本構成にはこれらも含めて構築する。<br>
Infra Node と Storage Node は、最初 Worker Node として立ち上げ、クラスター構築後にラベルを貼ることでそれぞれの役割となる。
- また、クラスター構築時に Master Node を起動するための Bootstrap Node が一時的に必要となる。<br>
Master Node が立ち上がれば Bootstrap Node は不要となるので、別の役割のサーバーとして切り替える事ができる。

<br>

### 3.1.2. 各ノードのサイジング
#### Master Node
- Master Nodeは **3台** が必要
- [Master Nodeの最小リソース要件](https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.5/html/installing_on_bare_metal/installing-on-bare-metal#installation-requirements-user-infra_installing-bare-metal)
- [Master Nodeの推奨スペック](https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.5/html/scalability_and_performance/master-node-sizing_) <br>
管理対象のWorker Nodeの数で変わる。
Master Nodeはスケールアウトしたり、CPUやRAMのサイズを変更することができないため、あらかじめクラスターに配備するWorker Nodeの最大数を想定してスペックを決める。 <br>

#### Worker Node
- Worker Nodeは **2台以上** が必要 <br>
本構成では3つのFailure Domainで分散するため3台とする。
- [Worker Nodeの最小リソース要件](https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.5/html/installing_on_bare_metal/installing-on-bare-metal#installation-requirements-user-infra_installing-bare-metal)
- Worker Nodeの推奨スペック <br>
実際にはWorker Nodeで稼働するアプリケーションPodが求めるリソースに依存するので、一概に推奨スペックは言えない。  
あらかじめ稼働するアプリケーションが全て分かっている場合は必要なリソースが計算できるが、分からない場合は暫定的にスペックを決めスケールアウトする方針がよい。

#### Infra Node
- Infra Nodeは **3台以上** が必要 <br>
本構成では3台で構成する。
- Infra Nodeの最小リソース要件はWorker Nodeに則するが、最小要件ではリソース不足でクラスターサービスが稼働しない恐れがある。
推奨スペックのリソースを用意すること。
- [Infra Nodeの推奨スペック](https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.5/html/scalability_and_performance/infrastructure-node-sizing_)<br>
管理対象のWorker Nodeの数で変わる。


#### Storage Node (OCS Node)
- Storage Nodeは **3台以上** が必要 <br>
本構成では3台で構成する。
- [Storage Nodeの最小リソース要件](https://access.redhat.com/documentation/en-us/red_hat_openshift_container_storage/4.5/html-single/planning_your_deployment/index#resource-requirements_rhocs)<br>
ドライブが増えるごとに追加でリソースが必要になることに注意。
- 4 TiB以下のSSDをドライブとして使用できる。
- OpenShift Container Storageでは、3 Nodeにまたがって三重でレプリケーションして冗長化する。
したがって、3 Node全てにPersistent Storageとして使用する容量分のドライブを搭載すること。

#### Bootstrap Node
- Bootstrap Nodeは **1台** が必要 <br>
- [Bootstrap Nodeの最小リソース要件](https://access.redhat.com/documentation/ja-jp/openshift_container_platform/4.5/html/installing_on_bare_metal/installing-on-bare-metal#installation-requirements-user-infra_installing-bare-metal)

<br>
